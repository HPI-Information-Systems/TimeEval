{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numenta Anomaly Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Final\n",
    "from collections.abc import Callable\n",
    "from datetime import datetime\n",
    "from config import data_raw_folder, data_processed_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_collection_name = \"NAB\"\n",
    "source_folder = os.path.join(data_raw_folder, \"Community-NAB\")\n",
    "target_folder = data_processed_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "DatasetMetadataRecord = {\n",
    "    \"dataset_name\": str,\n",
    "    \"collection_name\": str,\n",
    "    \"train_path\": str,\n",
    "    \"test_path\": str,\n",
    "    \"dataset_type\": str,\n",
    "    \"datetime_index\": bool,\n",
    "    \"split_at\": int,\n",
    "    \"train_type\": str,\n",
    "    \"train_is_normal\": bool,\n",
    "    \"input_type\": str,\n",
    "    \"length\": int\n",
    "}\n",
    "\n",
    "class DatasetMetadata:\n",
    "    \"\"\"\n",
    "    ATTENTION: Not thread-safe! There is no check for changes to the underlying `dataset.csv` file while this class is loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    FILENAME: Final[str] = \"datasets.csv\"\n",
    "    \n",
    "    _filepath: str\n",
    "    _df: pd.DataFrame\n",
    "    _dirty: bool\n",
    "\n",
    "    def __init__(self, target_folder: str):\n",
    "        self._filepath = os.path.join(target_folder, self.FILENAME)\n",
    "        self._dirty = False\n",
    "        if not os.path.isfile(self._filepath):\n",
    "            self._df = self._create_metadata_file()\n",
    "        else:\n",
    "            self.refresh(force = True)\n",
    "    \n",
    "    def __enter__(self) -> 'DatasetMetadata':\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, exception_traceback) -> 'DatasetMetadata':\n",
    "        self.save()\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return repr(self._df)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self._df)\n",
    "        \n",
    "    def _create_metadata_file(self) -> pd.DataFrame:\n",
    "        df_temp = pd.DataFrame(columns=[\"dataset_name\", \"collection_name\", \"train_path\", \"test_path\", \"type\", \"datetime_index\", \"split_at\", \"train_type\", \"train_is_normal\", \"input_type\", \"length\"])\n",
    "        df_temp.set_index([\"dataset_name\", \"collection_name\"], inplace=True)\n",
    "        df_temp.to_csv(self._filepath)\n",
    "        return df_temp\n",
    "    \n",
    "    def add_dataset(self,\n",
    "        dataset_name: str,\n",
    "        collection_name: str,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        dataset_type: str,\n",
    "        datetime_index: bool,\n",
    "        split_at: int,\n",
    "        train_type: str,\n",
    "        train_is_normal: bool,\n",
    "        input_type: str,\n",
    "        dataset_length: int\n",
    "    ) -> 'DatasetMetadata':\n",
    "        df_new = pd.DataFrame({\n",
    "            \"train_path\": train_path,\n",
    "            \"test_path\": test_path,\n",
    "            \"type\": dataset_type,\n",
    "            \"datetime_index\": datetime_index,\n",
    "            \"split_at\": split_at,\n",
    "            \"train_type\": train_type,\n",
    "            \"train_is_normal\": train_is_normal,\n",
    "            \"input_type\": input_type,\n",
    "            \"length\": dataset_length\n",
    "        }, index=[(dataset_name, dataset_collection_name)])\n",
    "        df = pd.concat([self._df, df_new], axis=0)\n",
    "        df = df[~df.index.duplicated(keep = \"last\")]\n",
    "        self._df = df\n",
    "        self._dirty = True\n",
    "        return self\n",
    "    \n",
    "    def add_datasets(self, datasets: list[DatasetMetadataRecord]) -> 'DatasetMetadata':\n",
    "        df_new = pd.DataFrame(datasets)\n",
    "        df_new.set_index([\"dataset_name\", \"collection_name\"], inplace = True)\n",
    "        df = pd.concat([self._df, df_new], axis=0)\n",
    "        df = df[~df.index.duplicated(keep = \"last\")]\n",
    "        self._df = df\n",
    "        self._dirty = True\n",
    "        return self\n",
    "    \n",
    "    def refresh(self, force: bool = False) -> None:\n",
    "        if not force and self._dirty:\n",
    "            raise Exception(\"There are unsaved changes in memory that would get lost by reading from disk again!\")\n",
    "        else:\n",
    "            self._df = pd.read_csv(self._filepath, index_col=[\"dataset_name\", \"collection_name\"])\n",
    "    \n",
    "    def save(self) -> None:\n",
    "        self._df.to_csv(self._filepath)\n",
    "        self._dirty = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_size(filename: str) -> int:\n",
    "    with open(filename, 'r') as f:\n",
    "        next(f) # skips header\n",
    "        c = 0\n",
    "        for line in f:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def transform_and_label(source: str, target: str, anomaly_windows: list[str]) -> None:\n",
    "    df = pd.read_csv(source)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)\n",
    "    df[\"is_anomaly\"] = 0\n",
    "\n",
    "    for t1, t2 in anomaly_windows:\n",
    "        t1 = datetime.strptime(t1, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        t2 = datetime.strptime(t2, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        moreThanT1 = df[df[\"timestamp\"] >= t1]\n",
    "        betweenT1AndT2 = moreThanT1[moreThanT1[\"timestamp\"] <= t2]\n",
    "        indices = betweenT1AndT2.index\n",
    "        df[\"is_anomaly\"].values[indices.values] = 1\n",
    "\n",
    "    df.to_csv(target, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories data-processed/univariate/NAB already exist\n"
     ]
    }
   ],
   "source": [
    "# shared by all datasets\n",
    "input_type = \"univariate\"\n",
    "datetime_index = True\n",
    "train_type = \"unsupervised\"\n",
    "train_is_normal = False\n",
    "\n",
    "dm = DatasetMetadata(target_folder)\n",
    "\n",
    "# create target directory\n",
    "dataset_subfolder = os.path.join(target_folder, input_type, dataset_collection_name)\n",
    "try:\n",
    "    os.makedirs(dataset_subfolder)\n",
    "    print(f\"Created directories {dataset_subfolder}\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directories {dataset_subfolder} already exist\")\n",
    "    pass\n",
    "\n",
    "with open(os.path.join(source_folder, \"labels\", \"combined_windows.json\"), 'r') as f:\n",
    "    windows = json.load(f)\n",
    "\n",
    "#windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data-raw/Community-NAB/data/artificialNoAnomaly/art_daily_no_noise.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-136-6b68120cba11>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mdataset_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset_subfolder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mdataset_length\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalc_size\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msource_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     dm.add_dataset(\n\u001B[1;32m     17\u001B[0m         \u001B[0mdataset_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-104-8db8c1f875af>\u001B[0m in \u001B[0;36mcalc_size\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcalc_size\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m         \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# skips header\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data-raw/Community-NAB/data/artificialNoAnomaly/art_daily_no_noise.csv'"
     ]
    }
   ],
   "source": [
    "# dataset transformation\n",
    "transform_file: Callable[[str, str, list[str]], None] = transform_and_label\n",
    "\n",
    "for dataset in windows:\n",
    "    source_file = os.path.join(source_folder, \"data\", dataset)\n",
    "    dataset_type = \"real\" if dataset.startswith(\"real\") else \"synthetic\"\n",
    "    \n",
    "    # get basename for target filename\n",
    "    basename = os.path.splitext(os.path.basename(source_file))[0]\n",
    "    filename = f\"{basename}.test.csv\"\n",
    "\n",
    "    # save metadata\n",
    "    dataset_name = filename.split(\".\")[0]\n",
    "    path = os.path.join(dataset_subfolder, filename)\n",
    "    dataset_length = calc_size(source_file)\n",
    "    dm.add_dataset(\n",
    "        dataset_name = dataset_name,\n",
    "        collection_name = dataset_collection_name,\n",
    "        train_path = None,\n",
    "        test_path = path,\n",
    "        dataset_type = dataset_type,\n",
    "        datetime_index = datetime_index,\n",
    "        split_at = None,\n",
    "        train_type = train_type,\n",
    "        train_is_normal = train_is_normal,\n",
    "        input_type = input_type,\n",
    "        dataset_length = dataset_length\n",
    "    )\n",
    "    # transform file\n",
    "    transform_file(source_file, path, windows[dataset])\n",
    "    print(f\"Processed source dataset {source_file} -> {path}\")\n",
    "\n",
    "# save metadata of benchmark\n",
    "dm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_path</th>\n",
       "      <th>test_path</th>\n",
       "      <th>type</th>\n",
       "      <th>datetime_index</th>\n",
       "      <th>split_at</th>\n",
       "      <th>train_type</th>\n",
       "      <th>train_is_normal</th>\n",
       "      <th>input_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_name</th>\n",
       "      <th>collection_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1Benchmark-6</th>\n",
       "      <th>WebscopeS5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/WebscopeS5/A1Benchma...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1Benchmark-3</th>\n",
       "      <th>WebscopeS5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/WebscopeS5/A1Benchma...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1Benchmark-40</th>\n",
       "      <th>WebscopeS5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/WebscopeS5/A1Benchma...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1Benchmark-20</th>\n",
       "      <th>WebscopeS5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/WebscopeS5/A1Benchma...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1Benchmark-4</th>\n",
       "      <th>WebscopeS5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/WebscopeS5/A1Benchma...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter_volume_GOOG</th>\n",
       "      <th>NAB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/NAB/Twitter_volume_G...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>15842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter_volume_IBM</th>\n",
       "      <th>NAB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/NAB/Twitter_volume_I...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>15893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter_volume_KO</th>\n",
       "      <th>NAB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/NAB/Twitter_volume_K...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>15851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter_volume_PFE</th>\n",
       "      <th>NAB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/NAB/Twitter_volume_P...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>15858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter_volume_UPS</th>\n",
       "      <th>NAB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data-processed/univariate/NAB/Twitter_volume_U...</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>False</td>\n",
       "      <td>univariate</td>\n",
       "      <td>15866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     train_path  \\\n",
       "dataset_name        collection_name               \n",
       "A1Benchmark-6       WebscopeS5              NaN   \n",
       "A1Benchmark-3       WebscopeS5              NaN   \n",
       "A1Benchmark-40      WebscopeS5              NaN   \n",
       "A1Benchmark-20      WebscopeS5              NaN   \n",
       "A1Benchmark-4       WebscopeS5              NaN   \n",
       "...                                         ...   \n",
       "Twitter_volume_GOOG NAB                     NaN   \n",
       "Twitter_volume_IBM  NAB                     NaN   \n",
       "Twitter_volume_KO   NAB                     NaN   \n",
       "Twitter_volume_PFE  NAB                     NaN   \n",
       "Twitter_volume_UPS  NAB                     NaN   \n",
       "\n",
       "                                                                             test_path  \\\n",
       "dataset_name        collection_name                                                      \n",
       "A1Benchmark-6       WebscopeS5       data-processed/univariate/WebscopeS5/A1Benchma...   \n",
       "A1Benchmark-3       WebscopeS5       data-processed/univariate/WebscopeS5/A1Benchma...   \n",
       "A1Benchmark-40      WebscopeS5       data-processed/univariate/WebscopeS5/A1Benchma...   \n",
       "A1Benchmark-20      WebscopeS5       data-processed/univariate/WebscopeS5/A1Benchma...   \n",
       "A1Benchmark-4       WebscopeS5       data-processed/univariate/WebscopeS5/A1Benchma...   \n",
       "...                                                                                ...   \n",
       "Twitter_volume_GOOG NAB              data-processed/univariate/NAB/Twitter_volume_G...   \n",
       "Twitter_volume_IBM  NAB              data-processed/univariate/NAB/Twitter_volume_I...   \n",
       "Twitter_volume_KO   NAB              data-processed/univariate/NAB/Twitter_volume_K...   \n",
       "Twitter_volume_PFE  NAB              data-processed/univariate/NAB/Twitter_volume_P...   \n",
       "Twitter_volume_UPS  NAB              data-processed/univariate/NAB/Twitter_volume_U...   \n",
       "\n",
       "                                     type  datetime_index  split_at  \\\n",
       "dataset_name        collection_name                                   \n",
       "A1Benchmark-6       WebscopeS5       real            True       NaN   \n",
       "A1Benchmark-3       WebscopeS5       real            True       NaN   \n",
       "A1Benchmark-40      WebscopeS5       real            True       NaN   \n",
       "A1Benchmark-20      WebscopeS5       real            True       NaN   \n",
       "A1Benchmark-4       WebscopeS5       real            True       NaN   \n",
       "...                                   ...             ...       ...   \n",
       "Twitter_volume_GOOG NAB              real            True       NaN   \n",
       "Twitter_volume_IBM  NAB              real            True       NaN   \n",
       "Twitter_volume_KO   NAB              real            True       NaN   \n",
       "Twitter_volume_PFE  NAB              real            True       NaN   \n",
       "Twitter_volume_UPS  NAB              real            True       NaN   \n",
       "\n",
       "                                       train_type  train_is_normal  \\\n",
       "dataset_name        collection_name                                  \n",
       "A1Benchmark-6       WebscopeS5       unsupervised            False   \n",
       "A1Benchmark-3       WebscopeS5       unsupervised            False   \n",
       "A1Benchmark-40      WebscopeS5       unsupervised            False   \n",
       "A1Benchmark-20      WebscopeS5       unsupervised            False   \n",
       "A1Benchmark-4       WebscopeS5       unsupervised            False   \n",
       "...                                           ...              ...   \n",
       "Twitter_volume_GOOG NAB              unsupervised            False   \n",
       "Twitter_volume_IBM  NAB              unsupervised            False   \n",
       "Twitter_volume_KO   NAB              unsupervised            False   \n",
       "Twitter_volume_PFE  NAB              unsupervised            False   \n",
       "Twitter_volume_UPS  NAB              unsupervised            False   \n",
       "\n",
       "                                     input_type  length  \n",
       "dataset_name        collection_name                      \n",
       "A1Benchmark-6       WebscopeS5       univariate    1439  \n",
       "A1Benchmark-3       WebscopeS5       univariate    1461  \n",
       "A1Benchmark-40      WebscopeS5       univariate    1427  \n",
       "A1Benchmark-20      WebscopeS5       univariate    1422  \n",
       "A1Benchmark-4       WebscopeS5       univariate    1423  \n",
       "...                                         ...     ...  \n",
       "Twitter_volume_GOOG NAB              univariate   15842  \n",
       "Twitter_volume_IBM  NAB              univariate   15893  \n",
       "Twitter_volume_KO   NAB              univariate   15851  \n",
       "Twitter_volume_PFE  NAB              univariate   15858  \n",
       "Twitter_volume_UPS  NAB              univariate   15866  \n",
       "\n",
       "[425 rows x 9 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.refresh()\n",
    "dm._df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>is_anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-01 00:15:01</td>\n",
       "      <td>0.091795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-01 01:15:01</td>\n",
       "      <td>0.074414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-01 02:15:01</td>\n",
       "      <td>0.056984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-01 03:15:01</td>\n",
       "      <td>0.071225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-01 04:15:01</td>\n",
       "      <td>0.045466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>2011-09-07 10:15:01</td>\n",
       "      <td>0.054275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>2011-09-07 11:15:01</td>\n",
       "      <td>0.070650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>2011-09-07 12:15:01</td>\n",
       "      <td>0.056339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>2011-09-07 13:15:01</td>\n",
       "      <td>0.050782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>2011-09-07 14:15:01</td>\n",
       "      <td>0.056232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1643 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp     value  is_anomaly\n",
       "0    2011-07-01 00:15:01  0.091795           0\n",
       "1    2011-07-01 01:15:01  0.074414           0\n",
       "2    2011-07-01 02:15:01  0.056984           0\n",
       "3    2011-07-01 03:15:01  0.071225           0\n",
       "4    2011-07-01 04:15:01  0.045466           0\n",
       "...                  ...       ...         ...\n",
       "1638 2011-09-07 10:15:01  0.054275           0\n",
       "1639 2011-09-07 11:15:01  0.070650           0\n",
       "1640 2011-09-07 12:15:01  0.056339           0\n",
       "1641 2011-09-07 13:15:01  0.050782           0\n",
       "1642 2011-09-07 14:15:01  0.056232           0\n",
       "\n",
       "[1643 rows x 3 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"realAdExchange/exchange-4_cpc_results.csv\"\n",
    "source_file = os.path.join(source_folder, \"data\", dataset)\n",
    "df = pd.read_csv(source_file)\n",
    "df[\"timestamp\"] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)\n",
    "df[\"is_anomaly\"] = 0\n",
    "\n",
    "for t1, t2 in windows[dataset]:\n",
    "    t1 = datetime.strptime(t1, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    t2 = datetime.strptime(t2, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    moreThanT1 = df[df[\"timestamp\"] >= t1]\n",
    "    betweenT1AndT2 = moreThanT1[moreThanT1[\"timestamp\"] <= t2]\n",
    "    indices = betweenT1AndT2.index\n",
    "    df[\"is_anomaly\"].values[indices.values] = 1\n",
    "\n",
    "df[df[\"is_anomaly\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificialNoAnomaly/art_daily_no_noise.csv': [],\n",
       " 'artificialNoAnomaly/art_daily_perfect_square_wave.csv': [],\n",
       " 'artificialNoAnomaly/art_daily_small_noise.csv': [],\n",
       " 'artificialNoAnomaly/art_flatline.csv': [],\n",
       " 'artificialNoAnomaly/art_noisy.csv': [],\n",
       " 'artificialWithAnomaly/art_daily_flatmiddle.csv': ['2014-04-11 00:00:00'],\n",
       " 'artificialWithAnomaly/art_daily_jumpsdown.csv': ['2014-04-11 09:00:00'],\n",
       " 'artificialWithAnomaly/art_daily_jumpsup.csv': ['2014-04-11 09:00:00'],\n",
       " 'artificialWithAnomaly/art_daily_nojump.csv': ['2014-04-11 09:00:00'],\n",
       " 'artificialWithAnomaly/art_increase_spike_density.csv': ['2014-04-07 23:10:00'],\n",
       " 'artificialWithAnomaly/art_load_balancer_spikes.csv': ['2014-04-11 04:35:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv': ['2014-02-26 22:05:00',\n",
       "  '2014-02-27 17:15:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv': ['2014-02-19 19:10:00',\n",
       "  '2014-02-23 20:05:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv': ['2014-02-19 00:22:00',\n",
       "  '2014-02-24 18:37:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv': ['2014-04-09 10:15:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv': ['2014-04-15 15:44:00',\n",
       "  '2014-04-16 03:34:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv': ['2014-04-15 00:49:00'],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv': [],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv': ['2014-02-17 06:12:00',\n",
       "  '2014-02-22 00:02:00',\n",
       "  '2014-02-23 15:17:00'],\n",
       " 'realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv': ['2014-03-10 21:09:00'],\n",
       " 'realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv': ['2014-04-09 01:30:00',\n",
       "  '2014-04-10 14:35:00',\n",
       "  '2014-04-13 03:00:00'],\n",
       " 'realAWSCloudwatch/ec2_network_in_257a54.csv': ['2014-04-15 16:44:00'],\n",
       " 'realAWSCloudwatch/ec2_network_in_5abac7.csv': ['2014-03-10 18:56:00',\n",
       "  '2014-03-12 21:01:00'],\n",
       " 'realAWSCloudwatch/elb_request_count_8c0756.csv': ['2014-04-12 17:24:00',\n",
       "  '2014-04-22 19:34:00'],\n",
       " 'realAWSCloudwatch/grok_asg_anomaly.csv': ['2014-01-20 08:30:00',\n",
       "  '2014-01-21 10:45:00',\n",
       "  '2014-01-29 00:45:00'],\n",
       " 'realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv': ['2013-10-10 09:35:00',\n",
       "  '2013-10-10 20:40:00'],\n",
       " 'realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv': ['2014-02-25 07:15:00',\n",
       "  '2014-02-27 00:50:00'],\n",
       " 'realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv': ['2014-04-13 06:52:00',\n",
       "  '2014-04-18 23:27:00'],\n",
       " 'realAdExchange/exchange-2_cpc_results.csv': ['2011-07-14 13:00:01'],\n",
       " 'realAdExchange/exchange-2_cpm_results.csv': ['2011-07-26 06:00:01',\n",
       "  '2011-08-10 17:00:01'],\n",
       " 'realAdExchange/exchange-3_cpc_results.csv': ['2011-07-14 10:15:01',\n",
       "  '2011-07-20 10:15:01',\n",
       "  '2011-08-13 10:15:01'],\n",
       " 'realAdExchange/exchange-3_cpm_results.csv': ['2011-08-19 18:15:01'],\n",
       " 'realAdExchange/exchange-4_cpc_results.csv': ['2011-07-16 09:15:01',\n",
       "  '2011-08-02 12:15:01',\n",
       "  '2011-08-23 08:15:01'],\n",
       " 'realAdExchange/exchange-4_cpm_results.csv': ['2011-07-16 09:15:01',\n",
       "  '2011-08-01 07:15:01',\n",
       "  '2011-08-23 08:15:01',\n",
       "  '2011-08-28 13:15:01'],\n",
       " 'realKnownCause/ambient_temperature_system_failure.csv': ['2013-12-22 20:00:00',\n",
       "  '2014-04-13 09:00:00'],\n",
       " 'realKnownCause/cpu_utilization_asg_misconfiguration.csv': ['2014-07-12 02:04:00',\n",
       "  '2014-07-14 21:44:00'],\n",
       " 'realKnownCause/ec2_request_latency_system_failure.csv': ['2014-03-14 09:06:00',\n",
       "  '2014-03-18 22:41:00',\n",
       "  '2014-03-21 03:01:00'],\n",
       " 'realKnownCause/machine_temperature_system_failure.csv': ['2013-12-11 06:00:00',\n",
       "  '2013-12-16 17:25:00',\n",
       "  '2014-01-28 13:55:00',\n",
       "  '2014-02-08 14:30:00'],\n",
       " 'realKnownCause/nyc_taxi.csv': ['2014-11-01 19:00:00',\n",
       "  '2014-11-27 15:30:00',\n",
       "  '2014-12-25 15:00:00',\n",
       "  '2015-01-01 01:00:00',\n",
       "  '2015-01-27 00:00:00'],\n",
       " 'realKnownCause/rogue_agent_key_hold.csv': ['2014-07-15 08:30:00',\n",
       "  '2014-07-17 09:50:00'],\n",
       " 'realKnownCause/rogue_agent_key_updown.csv': ['2014-07-15 04:00:00',\n",
       "  '2014-07-17 08:50:00'],\n",
       " 'realTraffic/TravelTime_387.csv': ['2015-07-30 12:29:00',\n",
       "  '2015-08-18 16:26:00',\n",
       "  '2015-09-01 05:34:00'],\n",
       " 'realTraffic/TravelTime_451.csv': ['2015-08-11 12:07:00'],\n",
       " 'realTraffic/occupancy_6005.csv': ['2015-09-15 06:55:00'],\n",
       " 'realTraffic/occupancy_t4013.csv': ['2015-09-16 08:09:00',\n",
       "  '2015-09-17 07:55:00'],\n",
       " 'realTraffic/speed_6005.csv': ['2015-09-17 07:00:00'],\n",
       " 'realTraffic/speed_7578.csv': ['2015-09-11 16:44:00',\n",
       "  '2015-09-15 14:34:00',\n",
       "  '2015-09-16 14:14:00',\n",
       "  '2015-09-16 17:10:00'],\n",
       " 'realTraffic/speed_t4013.csv': ['2015-09-16 08:04:00', '2015-09-17 08:15:00'],\n",
       " 'realTweets/Twitter_volume_AAPL.csv': ['2015-03-03 21:07:53',\n",
       "  '2015-03-09 17:32:53',\n",
       "  '2015-03-16 02:57:53',\n",
       "  '2015-03-31 03:27:53'],\n",
       " 'realTweets/Twitter_volume_AMZN.csv': ['2015-03-05 19:47:53',\n",
       "  '2015-03-11 20:57:53',\n",
       "  '2015-04-01 21:57:53',\n",
       "  '2015-04-08 04:52:53'],\n",
       " 'realTweets/Twitter_volume_CRM.csv': ['2015-03-09 19:07:53',\n",
       "  '2015-03-19 23:07:53',\n",
       "  '2015-03-26 19:07:53'],\n",
       " 'realTweets/Twitter_volume_CVS.csv': ['2015-03-04 16:02:53',\n",
       "  '2015-03-05 19:57:53',\n",
       "  '2015-03-26 14:07:53',\n",
       "  '2015-04-14 22:37:53'],\n",
       " 'realTweets/Twitter_volume_FB.csv': ['2015-03-16 07:07:53',\n",
       "  '2015-04-03 17:47:53'],\n",
       " 'realTweets/Twitter_volume_GOOG.csv': ['2015-03-13 20:22:53',\n",
       "  '2015-03-14 16:27:53',\n",
       "  '2015-03-22 22:52:53',\n",
       "  '2015-04-01 05:27:53'],\n",
       " 'realTweets/Twitter_volume_IBM.csv': ['2015-03-23 22:27:53',\n",
       "  '2015-04-20 20:07:53'],\n",
       " 'realTweets/Twitter_volume_KO.csv': ['2015-03-20 13:12:53',\n",
       "  '2015-04-08 23:42:53',\n",
       "  '2015-04-14 14:52:53'],\n",
       " 'realTweets/Twitter_volume_PFE.csv': ['2015-03-02 21:22:53',\n",
       "  '2015-03-04 10:32:53',\n",
       "  '2015-03-13 19:57:53',\n",
       "  '2015-04-07 23:42:53'],\n",
       " 'realTweets/Twitter_volume_UPS.csv': ['2015-03-03 00:27:53',\n",
       "  '2015-03-04 11:07:53',\n",
       "  '2015-03-05 15:22:53',\n",
       "  '2015-03-24 18:17:53',\n",
       "  '2015-03-29 16:27:53']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data-raw/Community-NAB/labels/combined_labels.json\", 'r') as f:\n",
    "    labels = json.load(f)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificialNoAnomaly/art_daily_no_noise.csv': [],\n",
       " 'artificialNoAnomaly/art_daily_perfect_square_wave.csv': [],\n",
       " 'artificialNoAnomaly/art_daily_small_noise.csv': [],\n",
       " 'artificialNoAnomaly/art_flatline.csv': [],\n",
       " 'artificialNoAnomaly/art_noisy.csv': [],\n",
       " 'artificialWithAnomaly/art_daily_flatmiddle.csv': [['2014-04-10 07:15:00.000000',\n",
       "   '2014-04-11 16:45:00.000000']],\n",
       " 'artificialWithAnomaly/art_daily_jumpsdown.csv': [['2014-04-10 16:15:00.000000',\n",
       "   '2014-04-12 01:45:00.000000']],\n",
       " 'artificialWithAnomaly/art_daily_jumpsup.csv': [['2014-04-10 16:15:00.000000',\n",
       "   '2014-04-12 01:45:00.000000']],\n",
       " 'artificialWithAnomaly/art_daily_nojump.csv': [['2014-04-10 16:15:00.000000',\n",
       "   '2014-04-12 01:45:00.000000']],\n",
       " 'artificialWithAnomaly/art_increase_spike_density.csv': [['2014-04-07 06:25:00.000000',\n",
       "   '2014-04-08 15:55:00.000000']],\n",
       " 'artificialWithAnomaly/art_load_balancer_spikes.csv': [['2014-04-10 11:50:00.000000',\n",
       "   '2014-04-11 21:20:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv': [['2014-02-26 13:45:00.000000',\n",
       "   '2014-02-27 06:25:00.000000'],\n",
       "  ['2014-02-27 08:55:00.000000', '2014-02-28 01:35:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv': [['2014-02-19 10:50:00.000000',\n",
       "   '2014-02-20 03:30:00.000000'],\n",
       "  ['2014-02-23 11:45:00.000000', '2014-02-24 04:25:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_5f5533.csv': [['2014-02-18 16:02:00.000000',\n",
       "   '2014-02-19 08:42:00.000000'],\n",
       "  ['2014-02-24 10:17:00.000000', '2014-02-25 02:57:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv': [['2014-04-08 17:30:00.000000',\n",
       "   '2014-04-10 03:00:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv': [['2014-04-15 07:24:00.000000',\n",
       "   '2014-04-16 11:54:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv': [['2014-04-14 07:49:00.000000',\n",
       "   '2014-04-15 17:34:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_c6585a.csv': [],\n",
       " 'realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv': [['2014-02-17 00:37:00.000000',\n",
       "   '2014-02-17 11:47:00.000000'],\n",
       "  ['2014-02-21 18:27:00.000000', '2014-02-22 05:37:00.000000'],\n",
       "  ['2014-02-23 09:42:00.000000', '2014-02-23 20:52:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv': [['2014-03-10 01:29:00.000000',\n",
       "   '2014-03-11 16:49:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_disk_write_bytes_c0d644.csv': [['2014-04-08 19:55:00.000000',\n",
       "   '2014-04-09 07:05:00.000000'],\n",
       "  ['2014-04-10 09:00:00.000000', '2014-04-10 20:10:00.000000'],\n",
       "  ['2014-04-12 21:25:00.000000', '2014-04-13 08:35:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_network_in_257a54.csv': [['2014-04-14 23:59:00.000000',\n",
       "   '2014-04-16 09:29:00.000000']],\n",
       " 'realAWSCloudwatch/ec2_network_in_5abac7.csv': [['2014-03-10 09:06:00.000000',\n",
       "   '2014-03-11 04:46:00.000000'],\n",
       "  ['2014-03-12 11:11:00.000000', '2014-03-13 06:51:00.000000']],\n",
       " 'realAWSCloudwatch/elb_request_count_8c0756.csv': [['2014-04-12 09:04:00.000000',\n",
       "   '2014-04-13 01:44:00.000000'],\n",
       "  ['2014-04-22 11:14:00.000000', '2014-04-23 03:54:00.000000']],\n",
       " 'realAWSCloudwatch/grok_asg_anomaly.csv': [['2014-01-20 02:05:00.000000',\n",
       "   '2014-01-20 14:55:00.000000'],\n",
       "  ['2014-01-21 04:20:00.000000', '2014-01-21 17:10:00.000000'],\n",
       "  ['2014-01-28 18:20:00.000000', '2014-01-29 07:10:00.000000']],\n",
       " 'realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv': [['2013-10-10 10:35:00.000000',\n",
       "   '2013-10-10 15:45:00.000000'],\n",
       "  ['2013-10-10 18:05:00.000000', '2013-10-10 23:15:00.000000']],\n",
       " 'realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv': [['2014-02-24 22:50:00.000000',\n",
       "   '2014-02-25 15:35:00.000000'],\n",
       "  ['2014-02-26 16:30:00.000000', '2014-02-27 09:10:00.000000']],\n",
       " 'realAWSCloudwatch/rds_cpu_utilization_e47b3b.csv': [['2014-04-12 22:32:00.000000',\n",
       "   '2014-04-13 15:12:00.000000'],\n",
       "  ['2014-04-18 15:07:00.000000', '2014-04-19 07:47:00.000000']],\n",
       " 'realAdExchange/exchange-2_cpc_results.csv': [['2011-07-11 04:00:01.000000',\n",
       "   '2011-07-17 22:00:01.000000']],\n",
       " 'realAdExchange/exchange-2_cpm_results.csv': [['2011-07-24 14:00:01.000000',\n",
       "   '2011-07-27 22:00:01.000000'],\n",
       "  ['2011-08-09 00:00:01.000000', '2011-08-12 09:00:01.000000']],\n",
       " 'realAdExchange/exchange-3_cpc_results.csv': [['2011-07-13 09:15:01.000000',\n",
       "   '2011-07-15 11:15:01.000000'],\n",
       "  ['2011-07-19 09:15:01.000000', '2011-07-21 11:15:01.000000'],\n",
       "  ['2011-08-12 07:15:01.000000', '2011-08-14 13:15:01.000000']],\n",
       " 'realAdExchange/exchange-3_cpm_results.csv': [['2011-08-16 10:15:01.000000',\n",
       "   '2011-08-23 00:15:01.000000']],\n",
       " 'realAdExchange/exchange-4_cpc_results.csv': [['2011-07-15 06:15:01.000000',\n",
       "   '2011-07-17 12:15:01.000000'],\n",
       "  ['2011-08-01 07:15:01.000000', '2011-08-03 15:15:01.000000'],\n",
       "  ['2011-08-22 05:15:01.000000', '2011-08-24 11:15:01.000000']],\n",
       " 'realAdExchange/exchange-4_cpm_results.csv': [['2011-07-15 13:15:01.000000',\n",
       "   '2011-07-17 05:15:01.000000'],\n",
       "  ['2011-07-31 11:15:01.000000', '2011-08-02 05:15:01.000000'],\n",
       "  ['2011-08-22 12:15:01.000000', '2011-08-24 04:15:01.000000'],\n",
       "  ['2011-08-27 17:15:01.000000', '2011-08-29 09:15:01.000000']],\n",
       " 'realKnownCause/ambient_temperature_system_failure.csv': [['2013-12-15 07:00:00.000000',\n",
       "   '2013-12-30 09:00:00.000000'],\n",
       "  ['2014-03-29 15:00:00.000000', '2014-04-20 22:00:00.000000']],\n",
       " 'realKnownCause/cpu_utilization_asg_misconfiguration.csv': [['2014-07-10 12:29:00.000000',\n",
       "   '2014-07-15 17:19:00.000000']],\n",
       " 'realKnownCause/ec2_request_latency_system_failure.csv': [['2014-03-14 03:31:00.000000',\n",
       "   '2014-03-14 14:41:00.000000'],\n",
       "  ['2014-03-18 17:06:00.000000', '2014-03-19 04:16:00.000000'],\n",
       "  ['2014-03-20 21:26:00.000000', '2014-03-21 03:41:00.000000']],\n",
       " 'realKnownCause/machine_temperature_system_failure.csv': [['2013-12-10 06:25:00.000000',\n",
       "   '2013-12-12 05:35:00.000000'],\n",
       "  ['2013-12-15 17:50:00.000000', '2013-12-17 17:00:00.000000'],\n",
       "  ['2014-01-27 14:20:00.000000', '2014-01-29 13:30:00.000000'],\n",
       "  ['2014-02-07 14:55:00.000000', '2014-02-09 14:05:00.000000']],\n",
       " 'realKnownCause/nyc_taxi.csv': [['2014-10-30 15:30:00.000000',\n",
       "   '2014-11-03 22:30:00.000000'],\n",
       "  ['2014-11-25 12:00:00.000000', '2014-11-29 19:00:00.000000'],\n",
       "  ['2014-12-23 11:30:00.000000', '2014-12-27 18:30:00.000000'],\n",
       "  ['2014-12-29 21:30:00.000000', '2015-01-03 04:30:00.000000'],\n",
       "  ['2015-01-24 20:30:00.000000', '2015-01-29 03:30:00.000000']],\n",
       " 'realKnownCause/rogue_agent_key_hold.csv': [['2014-07-15 04:35:00.000000',\n",
       "   '2014-07-15 13:25:00.000000'],\n",
       "  ['2014-07-17 05:50:00.000000', '2014-07-18 06:45:00.000000']],\n",
       " 'realKnownCause/rogue_agent_key_updown.csv': [['2014-07-14 17:00:00.000000',\n",
       "   '2014-07-15 15:00:00.000000'],\n",
       "  ['2014-07-16 21:50:00.000000', '2014-07-17 19:50:00.000000']],\n",
       " 'realTraffic/TravelTime_387.csv': [['2015-07-27 10:56:00.000000',\n",
       "   '2015-07-31 18:01:00.000000'],\n",
       "  ['2015-08-17 11:51:00.000000', '2015-08-20 01:54:00.000000'],\n",
       "  ['2015-08-31 15:44:00.000000', '2015-09-01 13:06:00.000000']],\n",
       " 'realTraffic/TravelTime_451.csv': [['2015-08-09 17:57:00.000000',\n",
       "   '2015-08-12 20:01:00.000000']],\n",
       " 'realTraffic/occupancy_6005.csv': [['2015-09-14 17:45:00.000000',\n",
       "   '2015-09-15 17:24:00.000000']],\n",
       " 'realTraffic/occupancy_t4013.csv': [['2015-09-16 00:59:00.000000',\n",
       "   '2015-09-16 13:19:00.000000'],\n",
       "  ['2015-09-17 00:20:00.000000', '2015-09-17 13:10:00.000000']],\n",
       " 'realTraffic/speed_6005.csv': [['2015-09-16 18:40:00.000000',\n",
       "   '2015-09-17 16:24:00.000000']],\n",
       " 'realTraffic/speed_7578.csv': [['2015-09-11 15:34:00.000000',\n",
       "   '2015-09-11 17:54:00.000000'],\n",
       "  ['2015-09-15 13:26:00.000000', '2015-09-15 15:54:00.000000'],\n",
       "  ['2015-09-16 13:04:00.000000', '2015-09-16 15:20:00.000000'],\n",
       "  ['2015-09-16 16:00:00.000000', '2015-09-16 18:20:00.000000']],\n",
       " 'realTraffic/speed_t4013.csv': [['2015-09-16 00:44:00.000000',\n",
       "   '2015-09-16 13:14:00.000000'],\n",
       "  ['2015-09-17 00:30:00.000000', '2015-09-17 13:30:00.000000']],\n",
       " 'realTweets/Twitter_volume_AAPL.csv': [['2015-03-03 04:37:53.000000',\n",
       "   '2015-03-04 13:37:53.000000'],\n",
       "  ['2015-03-09 01:02:53.000000', '2015-03-10 10:02:53.000000'],\n",
       "  ['2015-03-15 10:27:53.000000', '2015-03-16 19:27:53.000000'],\n",
       "  ['2015-03-30 10:57:53.000000', '2015-03-31 19:57:53.000000']],\n",
       " 'realTweets/Twitter_volume_AMZN.csv': [['2015-03-05 03:22:53.000000',\n",
       "   '2015-03-06 12:12:53.000000'],\n",
       "  ['2015-03-11 04:32:53.000000', '2015-03-12 13:22:53.000000'],\n",
       "  ['2015-04-01 05:32:53.000000', '2015-04-02 14:22:53.000000'],\n",
       "  ['2015-04-07 12:27:53.000000', '2015-04-08 21:17:53.000000']],\n",
       " 'realTweets/Twitter_volume_CRM.csv': [['2015-03-08 21:02:53.000000',\n",
       "   '2015-03-10 17:12:53.000000'],\n",
       "  ['2015-03-19 01:02:53.000000', '2015-03-20 21:12:53.000000'],\n",
       "  ['2015-03-25 21:02:53.000000', '2015-03-27 17:12:53.000000']],\n",
       " 'realTweets/Twitter_volume_CVS.csv': [['2015-03-03 23:32:53.000000',\n",
       "   '2015-03-06 12:27:53.000000'],\n",
       "  ['2015-03-25 21:37:53.000000', '2015-03-27 06:37:53.000000'],\n",
       "  ['2015-04-14 06:07:53.000000', '2015-04-15 15:07:53.000000']],\n",
       " 'realTweets/Twitter_volume_FB.csv': [['2015-03-14 22:12:53.000000',\n",
       "   '2015-03-17 16:02:53.000000'],\n",
       "  ['2015-04-02 08:52:53.000000', '2015-04-05 02:42:53.000000']],\n",
       " 'realTweets/Twitter_volume_GOOG.csv': [['2015-03-13 03:52:53.000000',\n",
       "   '2015-03-15 08:57:53.000000'],\n",
       "  ['2015-03-22 06:22:53.000000', '2015-03-23 15:22:53.000000'],\n",
       "  ['2015-03-31 12:57:53.000000', '2015-04-01 21:57:53.000000']],\n",
       " 'realTweets/Twitter_volume_IBM.csv': [['2015-03-22 13:22:53.000000',\n",
       "   '2015-03-25 07:32:53.000000'],\n",
       "  ['2015-04-19 11:02:53.000000', '2015-04-22 05:12:53.000000']],\n",
       " 'realTweets/Twitter_volume_KO.csv': [['2015-03-19 15:12:53.000000',\n",
       "   '2015-03-21 11:12:53.000000'],\n",
       "  ['2015-04-08 01:42:53.000000', '2015-04-09 21:42:53.000000'],\n",
       "  ['2015-04-13 16:52:53.000000', '2015-04-15 12:52:53.000000']],\n",
       " 'realTweets/Twitter_volume_PFE.csv': [['2015-03-02 04:52:53.000000',\n",
       "   '2015-03-03 13:52:53.000000'],\n",
       "  ['2015-03-03 18:02:53.000000', '2015-03-05 03:02:53.000000'],\n",
       "  ['2015-03-13 03:27:53.000000', '2015-03-14 12:27:53.000000'],\n",
       "  ['2015-04-07 07:12:53.000000', '2015-04-08 16:12:53.000000']],\n",
       " 'realTweets/Twitter_volume_UPS.csv': [['2015-03-02 11:17:53.000000',\n",
       "   '2015-03-03 13:37:53.000000'],\n",
       "  ['2015-03-03 21:57:53.000000', '2015-03-05 00:17:53.000000'],\n",
       "  ['2015-03-05 02:12:53.000000', '2015-03-06 04:32:53.000000'],\n",
       "  ['2015-03-24 05:07:53.000000', '2015-03-25 07:27:53.000000'],\n",
       "  ['2015-03-29 03:17:53.000000', '2015-03-30 05:37:53.000000']]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data-raw/Community-NAB/labels/combined_windows.json\", 'r') as f:\n",
    "    windows = json.load(f)\n",
    "windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(str):\n",
    "    try:\n",
    "        return datetime.strptime(str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        return datetime.strptime(str, \"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'artificialNoAnomaly/art_daily_no_noise.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/opt/miniconda3/envs/akita-jupyter/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3079\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3080\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'artificialNoAnomaly/art_daily_no_noise.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-135-5837691402db>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmatches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mwindows\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0manomaly\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0manomaly_window\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwindows\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m         \u001B[0mmaybe_middle\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mto_datetime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manomaly\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;34m(\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mto_datetime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0manomaly_window\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/akita-jupyter/lib/python3.9/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3022\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3023\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3024\u001B[0;31m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3025\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3026\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/miniconda3/envs/akita-jupyter/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mget_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3080\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3081\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3082\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3083\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3084\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'artificialNoAnomaly/art_daily_no_noise.csv'"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "for dataset in windows:\n",
    "    for (anomaly, anomaly_window) in zip(labels[dataset], windows[dataset]):\n",
    "        maybe_middle = to_datetime(anomaly)\n",
    "        (start, end) = [to_datetime(d) for d in anomaly_window]\n",
    "        diff1 = maybe_middle - start\n",
    "        diff2 = end - maybe_middle\n",
    "        if diff1 == diff2:\n",
    "            # print(f\"{dataset}-{anomaly} is in the middle of anomaly window!\")\n",
    "            matches += 1\n",
    "        else:\n",
    "            print(dataset)\n",
    "            print(f\"{start} - ({diff1})- {anomaly} -({diff2})- {end}\")\n",
    "print(f\"matches: {matches}/{sum(list(map(lambda x: len(labels[x]), labels)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}